{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e430c844",
   "metadata": {},
   "source": [
    "# Lab 2: Multi-Layer Perceptrons, Backpropagation, and Evaluation\n",
    "\n",
    "**Module:** Artificial Intelligence  \n",
    "**Topic:** Neural Networks – Part 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e542a",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement a Multi-Layer Perceptron (MLP) with sigmoid activation from scratch\n",
    "2. Train the MLP using the backpropagation algorithm\n",
    "3. Apply data normalisation for neural network training\n",
    "4. Classify the Iris dataset using your MLP implementation\n",
    "5. Compute and interpret a confusion matrix\n",
    "6. Evaluate model performance using cross-validation\n",
    "7. Apply vector hashing for text classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f334f",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34d39f",
   "metadata": {},
   "source": [
    "### From Perceptrons to MLPs\n",
    "\n",
    "In Lab 1, we saw that single-layer perceptrons can only solve linearly separable problems. To solve more complex problems like XOR, we need **Multi-Layer Perceptrons (MLPs)** with hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd28d9",
   "metadata": {},
   "source": [
    "### The Sigmoid Activation Function\n",
    "\n",
    "Unlike the step function, the **sigmoid function** is differentiable, which allows us to use gradient-based training:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Its derivative (needed for backpropagation) is:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9103df",
   "metadata": {},
   "source": [
    "### The Backpropagation Algorithm\n",
    "\n",
    "Backpropagation trains MLPs by:\n",
    "\n",
    "1. **Forward pass:** Compute outputs layer by layer\n",
    "2. **Compute error:** Compare output to expected value\n",
    "3. **Backward pass:** Propagate error gradients back through the network\n",
    "4. **Update weights:** Adjust weights to reduce error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad8d57",
   "metadata": {},
   "source": [
    "### Data Normalisation\n",
    "\n",
    "Neural networks work best when input features are normalised to a consistent range (e.g., [0, 1] or [-1, 1]). This prevents features with larger values from dominating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d9d2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f8ce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f0e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement an MLP Class\n",
    "\n",
    "Build an MLP with:\n",
    "\n",
    "- Configurable number of input, hidden, and output neurons\n",
    "- Sigmoid activation function\n",
    "- Backpropagation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5869046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with one hidden layer.\n",
    "    Uses sigmoid activation and backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise the MLP.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input features\n",
    "        n_hidden : int\n",
    "            Number of hidden neurons\n",
    "        n_outputs : int\n",
    "            Number of output neurons\n",
    "        learning_rate : float\n",
    "            Learning rate for weight updates\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Input to hidden layer weights (weights_ih) and biases (bias_h)\n",
    "        self.weights_ih = np.random.uniform(-0.5, 0.5, (n_inputs, n_hidden))\n",
    "        self.bias_h = np.random.uniform(-0.5, 0.5, n_hidden)\n",
    "        \n",
    "        # Hidden to output layer weights (weights_ho) and biases (bias_o)\n",
    "        # TODO: add code here for weights_ho and bias_o\n",
    "        ## Hint: see code for weights_ih and bias_h above\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # TODO: Implement sigmoid\n",
    "        ## Hint: see Lab 1\n",
    "        # Clip to avoid overflow in exp\n",
    "        \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid: σ'(x) = σ(x) * (1 - σ(x)).\"\"\"\n",
    "        # TODO: Implement sigmoid derivative\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array : Output activations\n",
    "        \"\"\"\n",
    "        # TODO: code this entire method\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation).\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Input data\n",
    "        y : array-like, shape (n_samples, n_outputs)\n",
    "            Target outputs\n",
    "        \"\"\"\n",
    "        # TODO: code this entire method\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def train(self, X, y, epochs, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Training inputs\n",
    "        y : array-like, shape (n_samples, n_outputs)\n",
    "            Training targets\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        verbose : bool\n",
    "            Print progress if True\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list : Training loss history\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss (Mean Squared Error)\n",
    "            loss = np.mean((y - output) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Returns the class with highest output activation.\n",
    "        \"\"\"\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "print(\"MLP class defined successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
