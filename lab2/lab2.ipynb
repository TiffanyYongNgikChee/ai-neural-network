{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e430c844",
   "metadata": {},
   "source": [
    "# Lab 2: Multi-Layer Perceptrons, Backpropagation, and Evaluation\n",
    "\n",
    "**Module:** Artificial Intelligence  \n",
    "**Topic:** Neural Networks – Part 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e542a",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Implement a Multi-Layer Perceptron (MLP) with sigmoid activation from scratch\n",
    "2. Train the MLP using the backpropagation algorithm\n",
    "3. Apply data normalisation for neural network training\n",
    "4. Classify the Iris dataset using your MLP implementation\n",
    "5. Compute and interpret a confusion matrix\n",
    "6. Evaluate model performance using cross-validation\n",
    "7. Apply vector hashing for text classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f334f",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34d39f",
   "metadata": {},
   "source": [
    "### From Perceptrons to MLPs\n",
    "\n",
    "In Lab 1, we saw that single-layer perceptrons can only solve linearly separable problems. To solve more complex problems like XOR, we need **Multi-Layer Perceptrons (MLPs)** with hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd28d9",
   "metadata": {},
   "source": [
    "### The Sigmoid Activation Function\n",
    "\n",
    "Unlike the step function, the **sigmoid function** is differentiable, which allows us to use gradient-based training:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Its derivative (needed for backpropagation) is:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9103df",
   "metadata": {},
   "source": [
    "### The Backpropagation Algorithm\n",
    "\n",
    "Backpropagation trains MLPs by:\n",
    "\n",
    "1. **Forward pass:** Compute outputs layer by layer\n",
    "2. **Compute error:** Compare output to expected value\n",
    "3. **Backward pass:** Propagate error gradients back through the network\n",
    "4. **Update weights:** Adjust weights to reduce error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad8d57",
   "metadata": {},
   "source": [
    "### Data Normalisation\n",
    "\n",
    "Neural networks work best when input features are normalised to a consistent range (e.g., [0, 1] or [-1, 1]). This prevents features with larger values from dominating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d9d2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f0e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Implement an MLP Class\n",
    "\n",
    "Build an MLP with:\n",
    "\n",
    "- Configurable number of input, hidden, and output neurons\n",
    "- Sigmoid activation function\n",
    "- Backpropagation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5869046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with one hidden layer.\n",
    "    Uses sigmoid activation and backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialise the MLP.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input features\n",
    "        n_hidden : int\n",
    "            Number of hidden neurons\n",
    "        n_outputs : int\n",
    "            Number of output neurons\n",
    "        learning_rate : float\n",
    "            Learning rate for weight updates\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Input to hidden layer weights (weights_ih) and biases (bias_h)\n",
    "        self.weights_ih = np.random.uniform(-0.5, 0.5, (n_inputs, n_hidden))\n",
    "        self.bias_h = np.random.uniform(-0.5, 0.5, n_hidden)\n",
    "        \n",
    "        # Hidden to output layer weights (weights_ho) and biases (bias_o)\n",
    "        self.weights_ho = np.random.uniform(-0.5, 0.5, (n_hidden, n_outputs))\n",
    "        self.bias_o = np.random.uniform(-0.5, 0.5, n_outputs)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # TODO: Implement sigmoid\n",
    "        ## Hint: see Lab 1\n",
    "        # Clip to avoid overflow in exp\n",
    "        \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of sigmoid: σ'(x) = σ(x) * (1 - σ(x)).\"\"\"\n",
    "        # TODO: Implement sigmoid derivative\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array : Output activations\n",
    "        \"\"\"\n",
    "        # TODO: code this entire method\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backward pass (backpropagation).\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Input data\n",
    "        y : array-like, shape (n_samples, n_outputs)\n",
    "            Target outputs\n",
    "        \"\"\"\n",
    "        # TODO: code this entire method\n",
    "        ## Hint: see Lab 1\n",
    "        \n",
    "    def train(self, X, y, epochs, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_inputs)\n",
    "            Training inputs\n",
    "        y : array-like, shape (n_samples, n_outputs)\n",
    "            Training targets\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        verbose : bool\n",
    "            Print progress if True\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list : Training loss history\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss (Mean Squared Error)\n",
    "            loss = np.mean((y - output) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Returns the class with highest output activation.\n",
    "        \"\"\"\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "print(\"MLP class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fe3a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Test the MLP on XOR\n",
    "\n",
    "Before using the MLP on real data, verify it works on XOR (from Lab 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XOR data\n",
    "# TODO: add code here to define XOR data\n",
    "## Hint: see Lab 1\n",
    "\n",
    "# Create and train MLP\n",
    "np.random.seed(42)\n",
    "mlp_xor = MLP(n_inputs=2, n_hidden=4, n_outputs=1, learning_rate=2.0)\n",
    "losses_xor = mlp_xor.train(X_xor, y_xor, epochs=5000, verbose=False)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_xor)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('XOR Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nXOR Predictions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(X_xor)):\n",
    "    output = mlp_xor.forward(X_xor[i:i+1])\n",
    "    rounded = round(output[0][0])\n",
    "    correct = \"✓\" if rounded == y_xor[i][0] else \"✗\"\n",
    "    print(f\"Input: {X_xor[i]} -> Output: {output[0][0]:.4f} -> Rounded: {rounded} (Expected: {y_xor[i][0]}) {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce025a66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Load and Prepare the Iris Dataset\n",
    "\n",
    "The Iris dataset contains 150 samples of iris flowers with:\n",
    "\n",
    "- 4 features: sepal length, sepal width, petal length, petal width\n",
    "- 3 classes: Setosa, Versicolor, Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5c13b",
   "metadata": {},
   "source": [
    "### Task 3.1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Iris Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")\n",
    "print(f\"\\nClass distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb91de",
   "metadata": {},
   "source": [
    "### Task 3.2: Normalise the Data\n",
    "\n",
    "Normalise features to [0, 1] range for better neural network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_normalised = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Data Normalisation\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Before normalisation:\")\n",
    "print(f\"  Min: {X.min(axis=0)}\")\n",
    "print(f\"  Max: {X.max(axis=0)}\")\n",
    "print(\"\\nAfter normalisation:\")\n",
    "print(f\"  Min: {X_normalised.min(axis=0)}\")\n",
    "print(f\"  Max: {X_normalised.max(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584b4f9",
   "metadata": {},
   "source": [
    "### Task 3.3: One-Hot Encode the Targets\n",
    "\n",
    "For multi-class classification with neural networks, we convert class labels to one-hot encoding:\n",
    "\n",
    "- Class 0 → [1, 0, 0]\n",
    "- Class 1 → [0, 1, 0]\n",
    "- Class 2 → [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode targets\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(\"One-Hot Encoding\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original y[:5]: {y[:5]}\")\n",
    "print(f\"\\nOne-hot y[:5]:\")\n",
    "print(y_onehot[:5])\n",
    "print(f\"\\nEncoding: 0 → [1,0,0], 1 → [0,1,0], 2 → [0,0,1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce291d5",
   "metadata": {},
   "source": [
    "### Task 3.4: Split into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalised, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Also keep original labels for evaluation\n",
    "X_, y_train_labels, y_test_labels = train_test_split(\n",
    "    X_normalised, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train/Test Split\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8effdbbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea628165",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Train and Evaluate the MLP on Iris\n",
    "\n",
    "### Task 4.1: Create and Train the Network\n",
    "\n",
    "**Suggested architecture:**\n",
    "\n",
    "- 4 input neurons (one per feature)\n",
    "- 8 hidden neurons\n",
    "- 3 output neurons (one per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create MLP: 4 inputs, 8 hidden, 3 outputs, learning rate = 0.5\n",
    "# TODO: add code here to create MLP\n",
    "## Hint: see Exercise 2\n",
    "\n",
    "print(\"Training MLP on Iris dataset...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train\n",
    "losses_iris = mlp_iris.train(X_train, y_train, epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7705704",
   "metadata": {},
   "source": [
    "### Task 4.2: Plot Training Loss\n",
    "\n",
    "Visualise how the loss decreases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_iris)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Iris Classification - Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('iris_training_loss.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved as 'iris_training_loss.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06a52a",
   "metadata": {},
   "source": [
    "### Task 4.3: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = mlp_iris.predict(X_test)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(\"Test Set Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correct predictions: {np.sum(y_pred == y_true)}/{len(y_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe100db7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdab07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Confusion Matrix (From Scratch)\n",
    "\n",
    "A confusion matrix shows the counts of correct and incorrect predictions for each class.\n",
    "\n",
    "For a binary classifier:\n",
    "\n",
    "|            | Predicted + | Predicted - |\n",
    "|------------|-------------|-------------|\n",
    "| Actual +   | TP          | FN          |\n",
    "| Actual -   | FP          | TN          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc768c0d",
   "metadata": {},
   "source": [
    "### Task 5.1: Implement Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True class labels\n",
    "    y_pred : array-like\n",
    "        Predicted class labels\n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ndarray : Confusion matrix of shape (n_classes, n_classes)\n",
    "    \"\"\"\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cm[true, pred] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = compute_confusion_matrix(y_true, y_pred, n_classes=3)\n",
    "\n",
    "print(\"Confusion Matrix (from scratch)\")\n",
    "print(\"=\" * 50)\n",
    "print(cm)\n",
    "print(\"\\nRows = Actual, Columns = Predicted\")\n",
    "print(\"Diagonal = Correct predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c23065b",
   "metadata": {},
   "source": [
    "### Task 5.2: Verify with sklearn\n",
    "\n",
    "Compare your from-scratch implementation with sklearn's built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as sklearn_cm, classification_report\n",
    "\n",
    "# sklearn's confusion matrix (should match ours)\n",
    "cm_sklearn = sklearn_cm(y_true, y_pred)\n",
    "\n",
    "print(\"Verification with sklearn\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Our confusion matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nsklearn confusion matrix:\")\n",
    "print(cm_sklearn)\n",
    "print(f\"\\nMatrices match: {np.array_equal(cm, cm_sklearn)}\")\n",
    "\n",
    "# Full classification report\n",
    "print(\"\\n\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f90f8",
   "metadata": {},
   "source": [
    "### Task 5.3: Visualise the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a464e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names,\n",
    "           yticklabels=class_names,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "    \n",
    "    # Rotate tick labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                   ha=\"center\", va=\"center\",\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                   fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot\n",
    "fig = plot_confusion_matrix(cm, iris.target_names, \"Iris Classification - Confusion Matrix\")\n",
    "plt.savefig('iris_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved as 'iris_confusion_matrix.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b178c",
   "metadata": {},
   "source": [
    "### Task 5.4: Compute Performance Metrics\n",
    "\n",
    "From the confusion matrix, compute:\n",
    "\n",
    "- **Accuracy** = (TP + TN) / Total\n",
    "- **Precision** (per class) = TP / (TP + FP)\n",
    "- **Recall / Sensitivity** (per class) = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(cm):\n",
    "    \"\"\"Compute accuracy, precision, and recall from confusion matrix.\"\"\"\n",
    "    n_classes = cm.shape[0]\n",
    "    \n",
    "    # Accuracy: sum of diagonal / total\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    \n",
    "    # Per-class precision and recall\n",
    "    precision = np.zeros(n_classes)\n",
    "    recall = np.zeros(n_classes)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp  # Column sum minus TP\n",
    "        fn = np.sum(cm[i, :]) - tp  # Row sum minus TP\n",
    "        \n",
    "        precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Compute metrics\n",
    "accuracy, precision, recall = compute_metrics(cm)\n",
    "\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(f\"{'Class':<15} {'Precision':<15} {'Recall':<15}\")\n",
    "print(\"-\" * 30)\n",
    "for i, name in enumerate(iris.target_names):\n",
    "    print(f\"{name:<15} {precision[i]*100:>8.2f}% {recall[i]*100:>12.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4cf77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: Compare with sklearn's MLPClassifier\n",
    "\n",
    "Verify your implementation by comparing with sklearn's built-in MLP.\n",
    "\n",
    "**Note:** sklearn's MLPClassifier typically achieves higher accuracy because it includes:\n",
    "\n",
    "- Better weight initialisation (Xavier/Glorot)\n",
    "- Momentum to smooth gradient updates\n",
    "- Adaptive learning rate\n",
    "- L2 regularisation\n",
    "- Softmax output with cross-entropy loss (optimal for classification)\n",
    "\n",
    "Our simpler implementation demonstrates the core concepts but lacks these optimisations.\n",
    "\n",
    "sklearn also handles one-hot encoding internally when you pass class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74999d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn MLP\n",
    "sklearn_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(8,),        # One hidden layer with 8 neurons\n",
    "    activation='logistic',           # Sigmoid activation\n",
    "    solver='sgd',                    # Stochastic gradient descent\n",
    "    learning_rate_init=0.5,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train (sklearn receives class labels, not one-hot)\n",
    "sklearn_mlp.fit(X_train, y_train_labels)\n",
    "\n",
    "# Evaluate\n",
    "sklearn_pred = sklearn_mlp.predict(X_test)\n",
    "sklearn_accuracy = accuracy_score(y_test_labels, sklearn_pred)\n",
    "\n",
    "print(\"Comparison: Our MLP vs sklearn MLPClassifier\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Our MLP Accuracy:     {accuracy * 100:.2f}%\")\n",
    "print(f\"sklearn MLP Accuracy: {sklearn_accuracy * 100:.2f}%\")\n",
    "print(\"\\nsklearn typically achieves higher accuracy due to its optimisations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77361337",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 7: Cross-Validation\n",
    "\n",
    "Use 10-fold cross-validation to get a more robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sklearn MLP for cross-validation\n",
    "# TODO: add code here to create an sklearn MLP\n",
    "## Hint: see Exercise 6\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cv_scores = cross_val_score(sklearn_mlp_cv, X_normalised, y, cv=10)\n",
    "\n",
    "print(\"10-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"\\nMean accuracy: {cv_scores.mean() * 100:.2f}%\")\n",
    "print(f\"Standard deviation: {cv_scores.std() * 100:.2f}%\")\n",
    "print(f\"95% confidence interval: {cv_scores.mean()*100:.2f}% ± {cv_scores.std()*100*1.96:.2f}%\")\n",
    "\n",
    "# Visualise\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, 11), cv_scores * 100)\n",
    "plt.axhline(y=cv_scores.mean() * 100, color='r', linestyle='--', label=f'Mean: {cv_scores.mean()*100:.2f}%')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('10-Fold Cross-Validation Results')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('cross_validation_results.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
